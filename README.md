# Q-SAFe

Large Language Models (LLMs) have demonstrated strong potential in clinical reasoning, but their scale and cost prevent deployment on resource-constrained telemedicine edge networks. Small language models (SLMs) offer efficiency, yet their limited semantic depth hinders accurate diagnostic support. This gap motivates a Federated Learning (FL) approach, where distributed SLMs collaborate under supervision, while Knowledge Distillation (KD) transfers reasoning from LLMs to sustain accuracy. However, todayâ€™s SLM deployments still lack agentic coordination, leaving clinical workflows fragmented. Agentic communication security relies on classical cryptography that cannot withstand quantum adversaries. To address these challenges, we propose \textit{Q-SAFe}, a four-phase scheme that integrates federated SLM training, agentic orchestration, and Post-Quantum Cryptography (PQC) algorithms. Patient and hospital agents are registered, and are collaboratively trained with Federated Proximal (FedProx) and KD strategy. They are connected through authenticated cross-layer handshakes, and protected by Post Quantum Cryptography (PQC) scheme for secure record exchange. Validations on MIMIC-III show that \textit{Q-SAFe} sustains clinical-grade responsiveness ($\leq$ 200 ms latency under 300 users), achieves stable convergence ($\approx$ 85\% accuracy within 50 rounds), and imposes only 3 KB per-exchange cryptographic overhead, making it suitable for practical edge deployments.
